

**Performance**

**1. Time Taken by the System to Respond**
- **Description:** Measures the time taken by the system to respond to a user request (e.g., average page load time).
- **Derivation/Formula:** Measured in milliseconds (ms) or seconds (s). Average response time can be calculated over a defined period or for specific user actions.
- **Example:** The average response time for a login request is 2 seconds, indicating how quickly the system handles user authentication.

**2. Throughput**
- **Description:** Measures the number of transactions the system can handle per unit of time.
- **Derivation/Formula:** Measured in transactions per second (TPS) or similar units. Throughput can be measured under various load conditions.
- **Example:** The system can handle 1,000 transactions per second during peak usage, showcasing its capacity to process high volumes of transactions efficiently.

**3. Resource Utilization**
- **Description:** Measures how efficiently the system utilizes resources like CPU, memory, and network bandwidth (e.g., CPU usage percentage).
- **Derivation/Formula:** Measured as a percentage of available resources used.
- **Example:** CPU usage during peak load is typically around 70%, indicating effective management of processing power under high demand.

---

**Usability**

**4. Task Completion Time**
- **Description:** Measures the time taken by users to complete a specific task within the software.
- **Derivation/Formula:** Measured in seconds or minutes. Average task completion time can be calculated for various user groups and tasks.
- **Example:** The average time it takes users to complete a purchase is 3 minutes, demonstrating the efficiency of the user interface and workflow.

**5. Error Rate**
- **Description:** Measures the percentage of times users encounter errors while using the software.
- **Derivation/Formula:** Calculated as the number of errors divided by the total number of user actions, multiplied by 100%.
- **Example:** The error rate for searching products is 5%, reflecting the reliability of the search functionality.

**6. User Satisfaction Surveys**
- **Description:** Gathers feedback from users on their experience with the software's ease of use.
- **Derivation/Formula:** Conducted through surveys or interviews. Analyze responses to identify usability issues.
- **Example:** User satisfaction surveys reveal difficulty navigating the checkout process, highlighting areas for improvement in user experience.

---

**Security**

**7. Number of Vulnerabilities Identified**
- **Description:** Tracks the number of security vulnerabilities discovered during testing.
- **Derivation/Formula:** The total count of vulnerabilities identified through various security testing techniques.
- **Example:** 10 security vulnerabilities were identified during penetration testing, indicating areas needing security enhancements.

**8. Penetration Testing Results**
- **Description:** Measures the effectiveness of security controls against simulated attacks.
- **Derivation/Formula:** Analyzes the outcome of penetration testing to identify vulnerabilities exploited by attackers.
- **Example:** Penetration testing revealed a vulnerability allowing unauthorized access, demonstrating the need for improved security measures.

**9. Mean Time to Recover (MTTR)**
- **Description:** Measures the average time it takes to restore system functionality after a security breach.
- **Derivation/Formula:** Calculated as the total downtime after a security breach divided by the number of breaches.
- **Example:** The MTTR for security breaches is typically within 4 hours, indicating the team's efficiency in restoring services after an incident.

---

**Reliability**

**10. Uptime Percentage**
- **Description:** Measures the percentage of time the system is operational and accessible to users.
- **Derivation/Formula:** Calculated as the total uptime divided by the total monitoring period, multiplied by 100%.
- **Example:** The system uptime for the past month was 99.5%, demonstrating high reliability and availability.

**11. Mean Time Between Failures (MTBF)**
- **Description:** Measures the average time between system failures.
- **Derivation/Formula:** Calculated as the total uptime divided by the number of failures during that period.
- **Example:** The MTBF for the server is 24 hours, indicating the average time the system operates without interruption.

**12. Defect Escape Rate**
- **Description:** Measures the percentage of defects that remain undetected after testing and reach production.
- **Derivation/Formula:** Calculated as the number of defects found in production divided by the total number of defects identified during testing, multiplied by 100%.
- **Example:** The defect escape rate for this release is 2%, showing the effectiveness of the testing process in catching defects before release.

---

**Scalability**

**13. Time to Scale**
- **Description:** Measures the time it takes for the system to adapt to increased user load or data volume.
- **Derivation/Formula:** Measured in minutes or hours.
- **Example:** The system can scale to handle additional users within 10 minutes, demonstrating its ability to quickly adapt to changing demands.

**14. Performance Under Load**
- **Description:** Measures how well the system performs under increased load conditions.
- **Derivation/Formula:** Measured through performance metrics like response time and throughput under simulated load scenarios.
- **Example:** System performance remains stable under increased load with minimal response time increase, indicating robustness under stress.

---

**Compatibility**

**15. Number of Supported Platforms**
- **Description:** Tracks the number of operating systems and devices the software is compatible with.
- **Derivation/Formula:** Maintained list of compatible platforms through testing on various systems.
- **Example:** The software is compatible with Windows 10, macOS 12, and major mobile phone operating systems, ensuring broad usability.

**16. Compatibility Testing Results**
- **Description:** Measures how well the software functions on different platforms and browsers.
- **Derivation/Formula:** Analyzes functionality and performance across various platforms and browsers.
- **Example:** Compatibility testing revealed a minor display issue on a specific browser version, which will be addressed, ensuring a consistent user experience.

---

**Additional Non-Functional Metrics**

**17. Load Handling Capability**
- **Description:** Measures the system's capability to handle a specific load without performance degradation.
- **Derivation/Formula:** Analyzes system performance metrics under defined load conditions.
- **Example:** The system can handle 10,000 concurrent users without a significant increase in response time, indicating excellent load handling capability.

**18. Stress Tolerance**
- **Description:** Measures the system's ability to maintain functionality under extreme conditions.
- **Derivation/Formula:** Monitors system performance under high load conditions until the system fails.
- **Example:** Stress testing shows that the system maintains stability up to 15,000 concurrent users, beyond which performance degrades, identifying the system's breaking point.

**19. Recovery Time Objective (RTO)**
- **Description:** The maximum acceptable amount of time to restore a system after a failure.
- **Derivation/Formula:** Defined as part of the disaster recovery plan and measured during recovery testing.
- **Example:** The RTO for critical systems is 1 hour, ensuring minimal disruption in case of a failure.

**20. Disaster Recovery Readiness**
- **Description:** Measures the preparedness of the system and processes to recover from a disaster.
- **Derivation/Formula:** Evaluated through regular disaster recovery drills and tests.
- **Example:** Disaster recovery tests show that all critical systems can be restored within the specified RTO, confirming readiness.

